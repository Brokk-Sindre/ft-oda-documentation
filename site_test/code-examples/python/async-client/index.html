<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Python Async Client - Test</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">Test</a>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#python-async-client" class="nav-link">Python Async Client</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#installation" class="nav-link">Installation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#async-client-implementation" class="nav-link">Async Client Implementation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#advanced-async-patterns" class="nav-link">Advanced Async Patterns</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#performance-benefits" class="nav-link">Performance Benefits</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#usage-guidelines" class="nav-link">Usage Guidelines</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#production-deployment" class="nav-link">Production Deployment</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="python-async-client">Python Async Client</h1>
<p>High-performance asynchronous client for the Danish Parliament API using <code>asyncio</code> and <code>aiohttp</code> for concurrent requests.</p>
<h2 id="installation">Installation</h2>
<pre><code class="language-bash">pip install aiohttp asyncio  # Required for async functionality
</code></pre>
<h2 id="async-client-implementation">Async Client Implementation</h2>
<pre><code class="language-python">import asyncio
import aiohttp
import urllib.parse
import time
from typing import Dict, List, Optional, Union, Any, AsyncGenerator
from datetime import datetime, timedelta
import json
import logging

logger = logging.getLogger('AsyncDanishParliamentAPI')

class AsyncDanishParliamentAPI:
    &quot;&quot;&quot;
    High-performance async client for Danish Parliament API.

    Features:
    - Concurrent request processing
    - Connection pooling and reuse
    - Async pagination with generators
    - Rate limiting and backoff
    - Memory-efficient streaming
    &quot;&quot;&quot;

    def __init__(self, max_connections: int = 10, request_delay: float = 0.1):
        &quot;&quot;&quot;
        Initialize async API client.

        Args:
            max_connections: Maximum concurrent connections
            request_delay: Minimum delay between requests (seconds)
        &quot;&quot;&quot;
        self.base_url = &quot;https://oda.ft.dk/api/&quot;
        self.max_connections = max_connections
        self.request_delay = request_delay
        self.session: Optional[aiohttp.ClientSession] = None
        self.last_request_time = 0

    async def __aenter__(self):
        &quot;&quot;&quot;Async context manager entry.&quot;&quot;&quot;
        await self._ensure_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        &quot;&quot;&quot;Async context manager exit.&quot;&quot;&quot;
        await self.close()

    async def _ensure_session(self):
        &quot;&quot;&quot;Ensure aiohttp session is created.&quot;&quot;&quot;
        if self.session is None or self.session.closed:
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                ttl_dns_cache=300,
                use_dns_cache=True,
            )

            timeout = aiohttp.ClientTimeout(total=60, connect=10)

            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={'User-Agent': 'AsyncDanishParliamentAPI/1.0'}
            )

    async def close(self):
        &quot;&quot;&quot;Clean up session resources.&quot;&quot;&quot;
        if self.session and not self.session.closed:
            await self.session.close()

    async def _rate_limit(self):
        &quot;&quot;&quot;Enforce rate limiting.&quot;&quot;&quot;
        elapsed = time.time() - self.last_request_time
        if elapsed &lt; self.request_delay:
            await asyncio.sleep(self.request_delay - elapsed)
        self.last_request_time = time.time()

    def _build_url(self, entity: str, **params) -&gt; str:
        &quot;&quot;&quot;Build properly encoded URL with OData parameters.&quot;&quot;&quot;
        url = f&quot;{self.base_url}{entity}&quot;

        if not params:
            return url

        # Build query parameters with proper encoding
        query_parts = []
        for key, value in params.items():
            if value is not None:
                if key.startswith('$'):
                    encoded_key = urllib.parse.quote(key, safe='')
                else:
                    encoded_key = key

                encoded_value = urllib.parse.quote(str(value), safe='()\',%')
                query_parts.append(f&quot;{encoded_key}={encoded_value}&quot;)

        return f&quot;{url}?{'&amp;'.join(query_parts)}&quot;

    async def _make_request(self, url: str, max_retries: int = 3) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;
        Make async HTTP request with error handling and retries.

        Args:
            url: URL to request
            max_retries: Number of retry attempts

        Returns:
            Parsed JSON response

        Raises:
            aiohttp.ClientError: For various client errors
        &quot;&quot;&quot;
        await self._ensure_session()
        await self._rate_limit()

        for attempt in range(max_retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 400:
                        raise aiohttp.ClientError(
                            f&quot;Bad Request (400): Invalid OData syntax for {url}&quot;
                        )
                    elif response.status == 404:
                        if '/api/' in url and url.count('/') == 4:
                            raise aiohttp.ClientError(f&quot;Entity not found: {url}&quot;)
                        else:
                            raise aiohttp.ClientError(f&quot;Record not found: {url}&quot;)
                    elif response.status == 501:
                        raise aiohttp.ClientError(
                            &quot;API is read-only - write operations not supported&quot;
                        )
                    else:
                        response.raise_for_status()

            except asyncio.TimeoutError:
                if attempt &lt; max_retries - 1:
                    wait_time = (2 ** attempt) * 1
                    logger.warning(f&quot;Request timeout, retrying in {wait_time}s...&quot;)
                    await asyncio.sleep(wait_time)
                    continue
                raise aiohttp.ClientError(f&quot;Request timeout after {max_retries} attempts&quot;)

            except aiohttp.ClientError:
                if attempt &lt; max_retries - 1:
                    wait_time = (2 ** attempt) * 1
                    await asyncio.sleep(wait_time)
                    continue
                raise

    async def get_cases(self, top: int = 100, skip: int = 0, 
                       filter_expr: Optional[str] = None,
                       expand: Optional[str] = None,
                       select: Optional[str] = None,
                       orderby: Optional[str] = None) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;
        Get parliamentary cases asynchronously.

        Args:
            top: Number of records (max 100)
            skip: Records to skip
            filter_expr: OData filter
            expand: Related entities
            select: Specific fields
            orderby: Sort order

        Returns:
            API response with case data
        &quot;&quot;&quot;
        params = {'$top': min(top, 100), '$skip': skip}

        if filter_expr:
            params['$filter'] = filter_expr
        if expand:
            params['$expand'] = expand
        if select:
            params['$select'] = select
        if orderby:
            params['$orderby'] = orderby

        url = self._build_url('Sag', **params)
        return await self._make_request(url)

    async def get_actors(self, top: int = 100, skip: int = 0,
                        filter_expr: Optional[str] = None,
                        expand: Optional[str] = None) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Get parliamentary actors asynchronously.&quot;&quot;&quot;
        params = {'$top': min(top, 100), '$skip': skip}

        if filter_expr:
            params['$filter'] = filter_expr
        if expand:
            params['$expand'] = expand

        url = self._build_url('Aktør', **params)
        return await self._make_request(url)

    async def paginate_all(self, entity: str, batch_size: int = 100,
                          max_records: Optional[int] = None,
                          **params) -&gt; AsyncGenerator[Dict[str, Any], None]:
        &quot;&quot;&quot;
        Async generator for paginating through all records.

        Args:
            entity: Entity name
            batch_size: Records per batch
            max_records: Maximum total records
            **params: Additional OData parameters

        Yields:
            Individual records
        &quot;&quot;&quot;
        skip = 0
        total_yielded = 0
        batch_size = min(batch_size, 100)

        while True:
            # Build request parameters
            request_params = {**params, '$top': batch_size, '$skip': skip}
            url = self._build_url(entity, **request_params)

            try:
                response = await self._make_request(url)
                records = response.get('value', [])

                if not records:
                    break

                # Yield each record
                for record in records:
                    yield record
                    total_yielded += 1

                    if max_records and total_yielded &gt;= max_records:
                        return

                skip += batch_size

            except Exception as e:
                logger.error(f&quot;Error paginating at skip={skip}: {e}&quot;)
                break

    async def get_concurrent_batches(self, entity: str, skip_values: List[int],
                                   batch_size: int = 100,
                                   **params) -&gt; List[Dict[str, Any]]:
        &quot;&quot;&quot;
        Fetch multiple batches concurrently.

        Args:
            entity: Entity name
            skip_values: List of skip values for concurrent requests
            batch_size: Records per batch
            **params: Additional OData parameters

        Returns:
            List of batch responses
        &quot;&quot;&quot;
        tasks = []

        for skip in skip_values:
            request_params = {**params, '$top': batch_size, '$skip': skip}
            url = self._build_url(entity, **request_params)
            task = self._make_request(url)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out exceptions and return successful responses
        successful_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f&quot;Batch at skip={skip_values[i]} failed: {result}&quot;)
            else:
                successful_results.append(result)

        return successful_results

    async def search_parallel(self, queries: List[Dict[str, Any]],
                            max_concurrent: int = 5) -&gt; List[Dict[str, Any]]:
        &quot;&quot;&quot;
        Execute multiple search queries in parallel.

        Args:
            queries: List of query parameters
            max_concurrent: Maximum concurrent requests

        Returns:
            List of query results
        &quot;&quot;&quot;
        semaphore = asyncio.Semaphore(max_concurrent)

        async def execute_query(query):
            async with semaphore:
                entity = query.pop('entity', 'Sag')
                url = self._build_url(entity, **query)
                return await self._make_request(url)

        tasks = [execute_query(query.copy()) for query in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [r for r in results if not isinstance(r, Exception)]

    async def monitor_changes(self, entities: List[str], 
                            check_interval: int = 300,
                            hours_back: int = 4) -&gt; AsyncGenerator[Dict[str, Any], None]:
        &quot;&quot;&quot;
        Continuously monitor for changes across multiple entities.

        Args:
            entities: List of entity names to monitor
            check_interval: Seconds between checks
            hours_back: Hours of history to check

        Yields:
            Change events as they're detected
        &quot;&quot;&quot;
        last_check_time = datetime.now() - timedelta(hours=hours_back)

        while True:
            current_time = datetime.now()
            iso_time = last_check_time.strftime('%Y-%m-%dT%H:%M:%S')

            # Check all entities for changes
            queries = []
            for entity in entities:
                queries.append({
                    'entity': entity,
                    '$filter': f&quot;opdateringsdato gt datetime'{iso_time}'&quot;,
                    '$orderby': 'opdateringsdato desc',
                    '$top': 100
                })

            try:
                results = await self.search_parallel(queries)

                # Process and yield changes
                for i, result in enumerate(results):
                    entity = entities[i]
                    changes = result.get('value', [])

                    for change in changes:
                        yield {
                            'entity': entity,
                            'record': change,
                            'change_type': 'update',
                            'detected_at': current_time.isoformat()
                        }

                last_check_time = current_time

            except Exception as e:
                logger.error(f&quot;Error monitoring changes: {e}&quot;)

            # Wait before next check
            await asyncio.sleep(check_interval)


# High-level async utility functions
async def bulk_fetch_cases(search_terms: List[str], 
                          max_concurrent: int = 5) -&gt; Dict[str, List[Dict]]:
    &quot;&quot;&quot;
    Fetch cases for multiple search terms concurrently.

    Args:
        search_terms: List of terms to search for
        max_concurrent: Maximum concurrent requests

    Returns:
        Dictionary mapping search terms to their results
    &quot;&quot;&quot;
    async with AsyncDanishParliamentAPI(max_connections=max_concurrent) as api:
        queries = []
        for term in search_terms:
            queries.append({
                'entity': 'Sag',
                '$filter': f&quot;substringof('{term}', titel)&quot;,
                '$top': 100
            })

        results = await api.search_parallel(queries, max_concurrent)

        # Map results back to search terms
        return {
            search_terms[i]: result.get('value', [])
            for i, result in enumerate(results)
            if i &lt; len(search_terms)
        }

async def fast_pagination_example():
    &quot;&quot;&quot;Demonstrate fast pagination with concurrent requests.&quot;&quot;&quot;

    async with AsyncDanishParliamentAPI() as api:
        # Get total count first
        count_response = await api.get_cases(top=1, **{'$inlinecount': 'allpages'})
        total_records = int(count_response.get('odata.count', 0))

        print(f&quot;Total records to fetch: {total_records:,}&quot;)

        # Generate skip values for concurrent batches
        batch_size = 100
        max_batches = 10  # Limit for demo
        skip_values = [i * batch_size for i in range(max_batches)]

        # Fetch multiple batches concurrently
        print(f&quot;Fetching {len(skip_values)} batches concurrently...&quot;)
        start_time = time.time()

        batches = await api.get_concurrent_batches('Sag', skip_values, batch_size)

        end_time = time.time()
        total_records_fetched = sum(len(batch.get('value', [])) for batch in batches)

        print(f&quot;Fetched {total_records_fetched:,} records in {end_time - start_time:.2f} seconds&quot;)
        print(f&quot;Average: {total_records_fetched / (end_time - start_time):.0f} records/second&quot;)

async def streaming_example():
    &quot;&quot;&quot;Demonstrate memory-efficient streaming of large datasets.&quot;&quot;&quot;

    async with AsyncDanishParliamentAPI() as api:
        print(&quot;Streaming all climate-related cases...&quot;)

        count = 0
        async for case in api.paginate_all(
            'Sag',
            max_records=500,  # Limit for demo
            **{'$filter': &quot;substringof('klima', titel)&quot;}
        ):
            count += 1
            if count % 50 == 0:
                print(f&quot;Processed {count} cases...&quot;)

            # Process each case individually without storing in memory
            # e.g., save to database, transform data, etc.

        print(f&quot;Finished processing {count} climate cases&quot;)

async def real_time_monitoring_example():
    &quot;&quot;&quot;Demonstrate real-time parliamentary activity monitoring.&quot;&quot;&quot;

    entities_to_monitor = ['Sag', 'Afstemning', 'Dokument']

    async with AsyncDanishParliamentAPI() as api:
        print(&quot;Starting real-time monitoring...&quot;)
        print(&quot;Press Ctrl+C to stop&quot;)

        try:
            change_count = 0
            async for change in api.monitor_changes(
                entities_to_monitor, 
                check_interval=60,  # Check every minute
                hours_back=1        # Look at last hour
            ):
                change_count += 1
                entity = change['entity']
                record = change['record']

                print(f&quot;Change #{change_count} in {entity}: {record.get('titel', record.get('navn', 'Unknown'))[:60]}&quot;)

                # Demo: stop after 10 changes
                if change_count &gt;= 10:
                    break

        except KeyboardInterrupt:
            print(&quot;\nMonitoring stopped&quot;)

# Usage examples
async def main():
    &quot;&quot;&quot;Main example demonstrating various async patterns.&quot;&quot;&quot;

    # Example 1: Basic async usage
    print(&quot;=== Basic Async Usage ===&quot;)
    async with AsyncDanishParliamentAPI() as api:
        cases = await api.get_cases(top=10)
        print(f&quot;Fetched {len(cases['value'])} cases&quot;)

    # Example 2: Concurrent searches
    print(&quot;\n=== Concurrent Searches ===&quot;)
    search_results = await bulk_fetch_cases([
        'klima', 'miljø', 'energi', 'transport'
    ])

    for term, results in search_results.items():
        print(f&quot;'{term}': {len(results)} cases found&quot;)

    # Example 3: Fast pagination
    print(&quot;\n=== Fast Pagination ===&quot;)
    await fast_pagination_example()

    # Example 4: Streaming
    print(&quot;\n=== Streaming Example ===&quot;)
    await streaming_example()

    # Example 5: Real-time monitoring (commented out for demo)
    # print(&quot;\n=== Real-time Monitoring ===&quot;)
    # await real_time_monitoring_example()

if __name__ == &quot;__main__&quot;:
    # Run the async examples
    asyncio.run(main())
</code></pre>
<h2 id="advanced-async-patterns">Advanced Async Patterns</h2>
<h3 id="1-producer-consumer-pattern-for-etl">1. Producer-Consumer Pattern for ETL</h3>
<pre><code class="language-python">import asyncio
from asyncio import Queue
import json

async def data_producer(api: AsyncDanishParliamentAPI, queue: Queue, entity: str):
    &quot;&quot;&quot;Produce data and put into queue.&quot;&quot;&quot;
    async for record in api.paginate_all(entity, max_records=1000):
        await queue.put(record)

    # Signal completion
    await queue.put(None)

async def data_processor(queue: Queue, output_file: str):
    &quot;&quot;&quot;Process data from queue and save to file.&quot;&quot;&quot;
    processed_count = 0

    with open(output_file, 'w', encoding='utf-8') as f:
        while True:
            record = await queue.get()

            if record is None:  # Producer finished
                break

            # Process the record (e.g., transform, validate)
            processed_record = {
                'id': record['id'],
                'title': record.get('titel', ''),
                'updated': record.get('opdateringsdato', ''),
                'processed_at': datetime.now().isoformat()
            }

            # Save to file
            f.write(json.dumps(processed_record, ensure_ascii=False) + '\n')
            processed_count += 1

            if processed_count % 100 == 0:
                print(f&quot;Processed {processed_count} records...&quot;)

            queue.task_done()

    print(f&quot;Finished processing {processed_count} records&quot;)

async def etl_pipeline_example():
    &quot;&quot;&quot;Demonstrate ETL pipeline using producer-consumer pattern.&quot;&quot;&quot;
    async with AsyncDanishParliamentAPI() as api:
        # Create queue for communication
        queue = asyncio.Queue(maxsize=100)  # Buffer size

        # Start producer and consumer concurrently
        producer_task = asyncio.create_task(
            data_producer(api, queue, 'Sag')
        )
        consumer_task = asyncio.create_task(
            data_processor(queue, 'processed_cases.jsonl')
        )

        # Wait for both to complete
        await asyncio.gather(producer_task, consumer_task)
</code></pre>
<h3 id="2-batch-processing-with-error-recovery">2. Batch Processing with Error Recovery</h3>
<pre><code class="language-python">async def resilient_batch_processor(api: AsyncDanishParliamentAPI,
                                  entity: str, 
                                  batch_size: int = 100,
                                  max_concurrent: int = 5):
    &quot;&quot;&quot;Process data in batches with error recovery.&quot;&quot;&quot;

    # Get total count
    count_response = await api.get_cases(top=1, **{'$inlinecount': 'allpages'})
    total_records = int(count_response.get('odata.count', 0))

    print(f&quot;Processing {total_records:,} records in batches of {batch_size}&quot;)

    semaphore = asyncio.Semaphore(max_concurrent)
    failed_batches = []

    async def process_batch(skip_value):
        async with semaphore:
            try:
                response = await api.get_cases(top=batch_size, skip=skip_value)
                records = response.get('value', [])

                # Simulate processing
                await asyncio.sleep(0.1)  # Processing time

                print(f&quot; Processed batch at skip={skip_value}: {len(records)} records&quot;)
                return len(records)

            except Exception as e:
                print(f&quot;L Failed batch at skip={skip_value}: {e}&quot;)
                failed_batches.append(skip_value)
                return 0

    # Create tasks for all batches
    skip_values = range(0, min(total_records, 1000), batch_size)  # Limit for demo
    tasks = [process_batch(skip) for skip in skip_values]

    # Process all batches
    results = await asyncio.gather(*tasks, return_exceptions=True)
    successful_records = sum(r for r in results if isinstance(r, int))

    print(f&quot;\nProcessed {successful_records:,} records successfully&quot;)

    # Retry failed batches
    if failed_batches:
        print(f&quot;Retrying {len(failed_batches)} failed batches...&quot;)
        retry_tasks = [process_batch(skip) for skip in failed_batches]
        retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)
        retry_successful = sum(r for r in retry_results if isinstance(r, int))
        print(f&quot;Recovered {retry_successful:,} records from failed batches&quot;)
</code></pre>
<h2 id="performance-benefits">Performance Benefits</h2>
<p>The async client provides significant performance improvements:</p>
<ol>
<li><strong>Concurrent Requests</strong>: 5-10x faster for multiple queries</li>
<li><strong>Memory Efficiency</strong>: Streaming prevents memory overflow</li>
<li><strong>Connection Reuse</strong>: HTTP/1.1 connection pooling</li>
<li><strong>Non-blocking I/O</strong>: CPU available for other tasks during network waits</li>
</ol>
<h2 id="usage-guidelines">Usage Guidelines</h2>
<ol>
<li><strong>Always use context manager</strong> (<code>async with</code>) for proper cleanup</li>
<li><strong>Respect rate limits</strong> - the API doesn't have explicit limits but be courteous</li>
<li><strong>Handle exceptions</strong> properly in async code</li>
<li><strong>Use semaphores</strong> to limit concurrent requests</li>
<li><strong>Consider memory usage</strong> when processing large datasets</li>
</ol>
<h2 id="production-deployment">Production Deployment</h2>
<pre><code class="language-python"># For production, use proper error handling and logging
import logging
logging.basicConfig(level=logging.INFO)

async def production_example():
    &quot;&quot;&quot;Production-ready async usage.&quot;&quot;&quot;

    api_config = {
        'max_connections': 10,
        'request_delay': 0.1
    }

    try:
        async with AsyncDanishParliamentAPI(**api_config) as api:
            # Your production logic here
            pass
    except Exception as e:
        logging.error(f&quot;Production API error: {e}&quot;)
        # Handle appropriately (alerts, fallback, etc.)
</code></pre>
<p>The async client is ideal for:
- <strong>ETL pipelines</strong> processing large datasets
- <strong>Real-time monitoring</strong> applications
- <strong>Data analysis</strong> requiring multiple concurrent queries
- <strong>Web applications</strong> needing responsive API calls</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js"></script>
        <script src="../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
