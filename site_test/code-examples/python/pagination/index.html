<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Python Pagination - Test</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">Test</a>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#python-pagination" class="nav-link">Python Pagination</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#overview" class="nav-link">Overview</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#key-limitations" class="nav-link">Key Limitations</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#basic-pagination" class="nav-link">Basic Pagination</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#generator-based-pagination" class="nav-link">Generator-Based Pagination</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#advanced-pagination-with-progress-tracking" class="nav-link">Advanced Pagination with Progress Tracking</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#parallel-pagination" class="nav-link">Parallel Pagination</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#best-practices" class="nav-link">Best Practices</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#performance-tips" class="nav-link">Performance Tips</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#error-handling-in-pagination" class="nav-link">Error Handling in Pagination</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="python-pagination">Python Pagination</h1>
<p>Comprehensive pagination strategies for handling large datasets from the Danish Parliament API.</p>
<h2 id="overview">Overview</h2>
<p>The Danish Parliament API has a hard limit of <strong>100 records per request</strong>. For large datasets, you need proper pagination strategies.</p>
<h2 id="key-limitations">Key Limitations</h2>
<ul>
<li><strong>Maximum records per request</strong>: 100 (enforced by API)</li>
<li><strong>Total dataset sizes</strong>: </li>
<li>Sag (Cases): 96,538+ records</li>
<li>Aktør (Actors): 18,139+ records</li>
<li>Stemme (Votes): Millions of records</li>
</ul>
<h2 id="basic-pagination">Basic Pagination</h2>
<pre><code class="language-python">import time
from danish_parliament_api import DanishParliamentAPI

def paginate_all_records(api, entity_name, batch_size=100, max_records=None):
    &quot;&quot;&quot;
    Fetch all records from an entity with pagination.

    Args:
        api: DanishParliamentAPI instance
        entity_name: Name of entity ('Sag', 'Aktør', etc.)
        batch_size: Records per request (max 100)
        max_records: Maximum total records to fetch

    Returns:
        List of all records
    &quot;&quot;&quot;
    all_records = []
    skip = 0
    batch_size = min(batch_size, 100)  # Enforce API limit

    while True:
        # Fetch batch
        if entity_name == 'Sag':
            response = api.get_cases(top=batch_size, skip=skip)
        elif entity_name == 'Aktør':
            response = api.get_actors(top=batch_size, skip=skip)
        else:
            # Generic approach
            url = api._build_url(entity_name, **{'$top': batch_size, '$skip': skip})
            response = api._make_request(url)

        records = response.get('value', [])

        # Check if we got any records
        if not records:
            break

        all_records.extend(records)
        print(f&quot;Fetched {len(records)} records, total: {len(all_records)}&quot;)

        # Check limits
        if max_records and len(all_records) &gt;= max_records:
            all_records = all_records[:max_records]
            break

        skip += batch_size

        # Be respectful to the API
        time.sleep(0.1)  # 100ms delay between requests

    return all_records

# Usage example
api = DanishParliamentAPI()

# Get all cases (will take ~17 minutes for full dataset)
print(&quot;Fetching all cases...&quot;)
all_cases = paginate_all_records(api, 'Sag', max_records=1000)  # Limit for demo
print(f&quot;Total cases fetched: {len(all_cases)}&quot;)
</code></pre>
<h2 id="generator-based-pagination">Generator-Based Pagination</h2>
<p>More memory-efficient approach using Python generators:</p>
<pre><code class="language-python">def paginate_records_generator(api, entity_name, batch_size=100, 
                             filter_expr=None, expand=None, select=None):
    &quot;&quot;&quot;
    Generator that yields records one by one with pagination.
    Memory efficient for very large datasets.

    Args:
        api: DanishParliamentAPI instance
        entity_name: Entity name
        batch_size: Records per request
        filter_expr: OData filter
        expand: Related entities to expand
        select: Fields to select

    Yields:
        Individual records
    &quot;&quot;&quot;
    skip = 0
    batch_size = min(batch_size, 100)

    while True:
        # Build parameters
        params = {'$top': batch_size, '$skip': skip}
        if filter_expr:
            params['$filter'] = filter_expr
        if expand:
            params['$expand'] = expand
        if select:
            params['$select'] = select

        # Fetch batch
        url = api._build_url(entity_name, **params)
        response = api._make_request(url)
        records = response.get('value', [])

        if not records:
            break

        # Yield each record
        for record in records:
            yield record

        skip += batch_size
        time.sleep(0.1)  # Rate limiting

# Usage with generator
api = DanishParliamentAPI()

print(&quot;Processing climate cases one by one...&quot;)
climate_count = 0
for case in paginate_records_generator(
    api, 'Sag', 
    filter_expr=&quot;substringof('klima', titel)&quot;
):
    climate_count += 1
    print(f&quot;Case {climate_count}: {case['titel'][:60]}...&quot;)

    # Process each case individually without storing all in memory
    # This is perfect for ETL pipelines or data processing

    if climate_count &gt;= 20:  # Demo limit
        break

print(f&quot;Processed {climate_count} climate cases&quot;)
</code></pre>
<h2 id="advanced-pagination-with-progress-tracking">Advanced Pagination with Progress Tracking</h2>
<pre><code class="language-python">from datetime import datetime
import json

class PaginationTracker:
    &quot;&quot;&quot;Track pagination progress and handle resumption.&quot;&quot;&quot;

    def __init__(self, entity_name, filename=None):
        self.entity_name = entity_name
        self.filename = filename or f&quot;{entity_name}_progress.json&quot;
        self.progress = self.load_progress()

    def load_progress(self):
        &quot;&quot;&quot;Load pagination progress from file.&quot;&quot;&quot;
        try:
            with open(self.filename, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {'skip': 0, 'total_fetched': 0, 'last_update': None}

    def save_progress(self):
        &quot;&quot;&quot;Save current progress to file.&quot;&quot;&quot;
        self.progress['last_update'] = datetime.now().isoformat()
        with open(self.filename, 'w') as f:
            json.dump(self.progress, f, indent=2)

    def update_progress(self, records_fetched):
        &quot;&quot;&quot;Update progress with new records.&quot;&quot;&quot;
        self.progress['skip'] += 100  # Standard batch size
        self.progress['total_fetched'] += records_fetched
        self.save_progress()

def paginate_with_resume(api, entity_name, total_expected=None, 
                        filter_expr=None, expand=None):
    &quot;&quot;&quot;
    Paginate with ability to resume from interruption.

    Args:
        api: DanishParliamentAPI instance
        entity_name: Entity to paginate
        total_expected: Expected total records (for progress)
        filter_expr: OData filter
        expand: Relationships to expand

    Returns:
        Generator yielding records with progress tracking
    &quot;&quot;&quot;
    tracker = PaginationTracker(entity_name)
    skip = tracker.progress['skip']

    print(f&quot;Resuming from record {skip:,}&quot;)
    if total_expected:
        print(f&quot;Progress: {skip:,} / {total_expected:,} ({skip/total_expected*100:.1f}%)&quot;)

    batch_size = 100
    consecutive_empty = 0

    while consecutive_empty &lt; 3:  # Stop after 3 empty responses
        params = {'$top': batch_size, '$skip': skip}
        if filter_expr:
            params['$filter'] = filter_expr
        if expand:
            params['$expand'] = expand

        try:
            url = api._build_url(entity_name, **params)
            response = api._make_request(url)
            records = response.get('value', [])

            if not records:
                consecutive_empty += 1
                skip += batch_size
                continue
            else:
                consecutive_empty = 0

            # Update progress
            tracker.update_progress(len(records))

            # Progress reporting
            if skip % 1000 == 0:  # Report every 1000 records
                print(f&quot;Processed {tracker.progress['total_fetched']:,} records...&quot;)
                if total_expected:
                    pct = tracker.progress['total_fetched'] / total_expected * 100
                    print(f&quot;Progress: {pct:.1f}%&quot;)

            # Yield records
            for record in records:
                yield record

            skip += batch_size
            time.sleep(0.1)  # Rate limiting

        except Exception as e:
            print(f&quot;Error at skip={skip}: {e}&quot;)
            print(&quot;Progress saved. You can resume later.&quot;)
            raise

# Usage with resume capability
api = DanishParliamentAPI()

# Get total count first for progress tracking
total_cases = api.get_entity_count('Sag')
print(f&quot;Total cases to process: {total_cases:,}&quot;)

# Process all cases with resume capability
processed = 0
try:
    for case in paginate_with_resume(api, 'Sag', total_expected=total_cases):
        processed += 1

        # Your processing logic here
        # e.g., save to database, analyze, transform, etc.

        if processed % 100 == 0:
            print(f&quot;Processed {processed:,} cases...&quot;)

        # Demo: stop after 500 records
        if processed &gt;= 500:
            break

except KeyboardInterrupt:
    print(f&quot;\nInterrupted after processing {processed:,} cases&quot;)
    print(&quot;Progress has been saved. Run again to resume.&quot;)
</code></pre>
<h2 id="parallel-pagination">Parallel Pagination</h2>
<p>For even faster data retrieval using concurrent requests:</p>
<pre><code class="language-python">import concurrent.futures
import threading
from queue import Queue

class ParallelPaginator:
    &quot;&quot;&quot;Fetch data using multiple parallel requests.&quot;&quot;&quot;

    def __init__(self, api, max_workers=5):
        self.api = api
        self.max_workers = max_workers
        self.results_queue = Queue()
        self.lock = threading.Lock()

    def fetch_batch(self, entity_name, skip, batch_size=100, **params):
        &quot;&quot;&quot;Fetch a single batch of records.&quot;&quot;&quot;
        try:
            # Add pagination params
            params.update({'$top': batch_size, '$skip': skip})

            url = self.api._build_url(entity_name, **params)
            response = self.api._make_request(url)
            records = response.get('value', [])

            with self.lock:
                print(f&quot;Fetched batch starting at {skip}: {len(records)} records&quot;)

            return skip, records

        except Exception as e:
            print(f&quot;Error fetching batch at {skip}: {e}&quot;)
            return skip, []

    def paginate_parallel(self, entity_name, total_records=None, 
                         batch_size=100, **filter_params):
        &quot;&quot;&quot;
        Paginate using parallel requests.

        Args:
            entity_name: Entity to fetch
            total_records: Total expected records (for batching)
            batch_size: Records per batch
            **filter_params: Additional OData parameters

        Returns:
            All records sorted by original order
        &quot;&quot;&quot;
        # Get total count if not provided
        if total_records is None:
            total_records = self.api.get_entity_count(entity_name)

        print(f&quot;Fetching {total_records:,} records in parallel...&quot;)

        # Create batch tasks
        batch_tasks = []
        for skip in range(0, total_records, batch_size):
            batch_tasks.append((skip, min(batch_size, total_records - skip)))

        # Execute in parallel
        all_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_skip = {
                executor.submit(
                    self.fetch_batch, entity_name, skip, size, **filter_params
                ): skip
                for skip, size in batch_tasks
            }

            # Collect results
            for future in concurrent.futures.as_completed(future_to_skip):
                skip, records = future.result()
                all_results.append((skip, records))

        # Sort by original order and flatten
        all_results.sort(key=lambda x: x[0])
        all_records = []
        for skip, records in all_results:
            all_records.extend(records)

        return all_records

# Usage
api = DanishParliamentAPI()
paginator = ParallelPaginator(api, max_workers=3)  # Be respectful

# Fetch climate cases in parallel
climate_cases = paginator.paginate_parallel(
    'Sag',
    total_records=500,  # Limit for demo
    **{'$filter': &quot;substringof('klima', titel)&quot;}
)

print(f&quot;Fetched {len(climate_cases)} climate cases in parallel&quot;)
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<h3 id="1-always-use-rate-limiting">1. Always Use Rate Limiting</h3>
<pre><code class="language-python">import time

def respectful_pagination(api, entity_name):
    &quot;&quot;&quot;Paginate with proper delays.&quot;&quot;&quot;
    skip = 0
    while True:
        response = api.get_cases(top=100, skip=skip)
        records = response.get('value', [])

        if not records:
            break

        # Process records...
        yield from records

        skip += 100
        time.sleep(0.1)  # 100ms delay - be respectful!
</code></pre>
<h3 id="2-handle-network-interruptions">2. Handle Network Interruptions</h3>
<pre><code class="language-python">def robust_pagination(api, entity_name):
    &quot;&quot;&quot;Paginate with error recovery.&quot;&quot;&quot;
    skip = 0
    consecutive_failures = 0

    while consecutive_failures &lt; 5:
        try:
            response = api.get_cases(top=100, skip=skip)
            records = response.get('value', [])

            if not records:
                break

            consecutive_failures = 0  # Reset on success
            yield from records
            skip += 100

        except Exception as e:
            consecutive_failures += 1
            wait_time = min(2 ** consecutive_failures, 60)  # Exponential backoff
            print(f&quot;Error: {e}. Retrying in {wait_time} seconds...&quot;)
            time.sleep(wait_time)
</code></pre>
<h3 id="3-memory-management-for-large-datasets">3. Memory Management for Large Datasets</h3>
<pre><code class="language-python">def memory_efficient_processing(api, entity_name):
    &quot;&quot;&quot;Process large datasets without storing all in memory.&quot;&quot;&quot;

    for batch_start in range(0, 100000, 100):  # Process in chunks
        response = api.get_cases(top=100, skip=batch_start)
        records = response.get('value', [])

        if not records:
            break

        # Process batch immediately
        for record in records:
            # Do your processing here
            process_single_record(record)

        # Clear references to help garbage collection
        del records, response

        # Optional: Force garbage collection for very large datasets
        import gc
        gc.collect()
</code></pre>
<h2 id="performance-tips">Performance Tips</h2>
<ol>
<li><strong>Use <code>$select</code></strong> to fetch only needed fields</li>
<li><strong>Avoid deep <code>$expand</code></strong> for better performance  </li>
<li><strong>Implement backoff</strong> for network errors</li>
<li><strong>Monitor API response times</strong> and adjust accordingly</li>
<li><strong>Cache results</strong> when appropriate</li>
<li><strong>Use parallel processing</strong> carefully (max 3-5 concurrent requests)</li>
</ol>
<h2 id="error-handling-in-pagination">Error Handling in Pagination</h2>
<pre><code class="language-python">def safe_paginate(api, entity_name, max_retries=3):
    &quot;&quot;&quot;Paginate with comprehensive error handling.&quot;&quot;&quot;
    skip = 0

    while True:
        retry_count = 0

        while retry_count &lt; max_retries:
            try:
                response = api.get_cases(top=100, skip=skip)
                records = response.get('value', [])

                if not records:
                    return  # No more data

                yield from records
                break  # Success - exit retry loop

            except NetworkError as e:
                retry_count += 1
                if retry_count &gt;= max_retries:
                    raise e
                print(f&quot;Network error, retrying... ({retry_count}/{max_retries})&quot;)
                time.sleep(2 ** retry_count)  # Exponential backoff

            except APIError as e:
                print(f&quot;API error: {e}&quot;)
                raise  # Don't retry API errors

        skip += 100
</code></pre>
<p>This comprehensive pagination system allows you to efficiently work with the entire Danish Parliament dataset while being respectful to the API and handling all edge cases.</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js"></script>
        <script src="../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
